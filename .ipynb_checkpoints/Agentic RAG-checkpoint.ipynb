{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5147b6da-30b0-4164-8cac-018b191a2fce",
   "metadata": {},
   "source": [
    "# Agentic RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752a3dff-4159-446e-b0cc-3c3c61a5ea61",
   "metadata": {},
   "source": [
    "### GET ENVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa42676e-01f3-4b5e-84ab-6f4c216df9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv_path = 'D:\\\\Dhananjay\\\\Pro\\\\DCode\\\\MyAIPractice\\\\.env'\n",
    "\n",
    "# import dotenv\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# Load environment variables\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "GEMINI_LLM_MODEL_NAME = os.getenv(\"GEMINI_LLM_MODEL_NAME\")\n",
    "GEMINI_EMBEDDING_MODEL_NAME = os.getenv(\"GEMINI_EMBEDDING_MODEL_NAME\")\n",
    "# GEMINI_LLM_MODEL_NAME = \"gemini-2.5-pro-exp-03-25\"\n",
    "# GEMINI_EMBEDDING_MODEL_NAME = \"gemini-embedding-exp-03-07\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03497fcb-2bb5-4d2e-a6b3-d6ff01247071",
   "metadata": {},
   "source": [
    "# Initialize Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8bbdce-5265-4578-b95d-6a05230b3ffd",
   "metadata": {},
   "source": [
    "### Initialize Embeddings Model (Google Gemini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abbfee5a-b939-4b89-acdb-d2beaefb8bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Embeddings Model (Google Gemini)\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "def get_embedding_model():\n",
    "    \"\"\"\n",
    "    Returns the configured embedding model for vector storage and retrieval.\n",
    "    \"\"\"\n",
    "    google_embedding = GoogleGenerativeAIEmbeddings(\n",
    "        model=GEMINI_EMBEDDING_MODEL_NAME,\n",
    "        google_api_key=GEMINI_API_KEY\n",
    "    )\n",
    "\n",
    "    return google_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dee081-3448-4d36-80a1-45d293c57e4a",
   "metadata": {},
   "source": [
    "### Initialize LLM (Google Gemini)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d313e66c-1ff8-4372-829d-b135e7255929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM (Google Gemini)\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "def get_llm_model():\n",
    "\n",
    "    \"\"\"\n",
    "    Returns the configured llm model for text generation.\n",
    "    \"\"\"\n",
    "    gemini_2_5 = ChatGoogleGenerativeAI(google_api_key=GEMINI_API_KEY, \n",
    "                                        model=GEMINI_LLM_MODEL_NAME,\n",
    "                                        temprature = 0,\n",
    "                                        streaming=True)\n",
    "\n",
    "    return gemini_2_5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92ef67c1-387c-4d58-b284-37c9fd6fed0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I am a large language model, trained by Google.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-be9694a4-868c-47ed-b0fd-d049a5a015f0-0', usage_metadata={'input_tokens': 4, 'output_tokens': 12, 'total_tokens': 16, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send compressed multipart ingest: Connection error caused failure to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. Please confirm your internet connection. ConnectionError(MaxRetryError('HTTPSConnectionPool(host=\\'api.smith.langchain.com\\', port=443): Max retries exceeded with url: /runs/multipart (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000002582E6EFAD0>: Failed to resolve \\'api.smith.langchain.com\\' ([Errno 11001] getaddrinfo failed)\"))'))\n",
      "Content-Length: 1011\n",
      "API Key: lsv2_********************************************98\n"
     ]
    }
   ],
   "source": [
    "get_llm_model().invoke(\"who are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d500f9a-2394-4dc9-aadc-276534d07832",
   "metadata": {},
   "source": [
    "## Load and Chunk Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d00eacc-55b0-4400-a33f-c459d2cd75ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import UnstructuredWordDocumentLoader\n",
    "\n",
    "\n",
    "def load_and_chunk_document(docx_path):\n",
    "    \"\"\"Loads document and intelligently chunks while preserving section integrity.\"\"\"\n",
    "    loader = UnstructuredWordDocumentLoader(docx_path, mode=\"elements\")\n",
    "    data = loader.load()\n",
    "\n",
    "    # Extract text and clean it\n",
    "    text = \"\\n\".join([chunk.page_content.strip() for chunk in data if chunk.page_content.strip()])\n",
    "\n",
    "    #  chunking\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,  # Large enough to capture a full clause\n",
    "        chunk_overlap=200,  # Overlap to prevent split clauses\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \"],  # Prefer splitting at paragraphs or sentences\n",
    "    )\n",
    "    \n",
    "    chunks = text_splitter.split_text(text)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Load and chunk document\n",
    "docx_path = \"contract_interview.docx\"\n",
    "document_chunks = load_and_chunk_document(docx_path)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9bbb73-8a84-4cf2-8fbb-d89c8fa8a01c",
   "metadata": {},
   "source": [
    "### Convert chunks to Langchain Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51b12230-57c7-45d1-bc0f-c7cc9ca7d479",
   "metadata": {},
   "outputs": [],
   "source": [
    " from langchain_core.documents import Document\n",
    "\n",
    "# Convert text chunks to LangChain Document objects\n",
    "documents = [Document(page_content=chunk) for chunk in document_chunks]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96f9a08-fd87-41e1-bc40-6645c83f4b15",
   "metadata": {},
   "source": [
    "# Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf025e8b-11fc-446e-9393-b800bfa8b9a1",
   "metadata": {},
   "source": [
    "First, we index the Docuemnts.\n",
    "\n",
    "Then we create a retriever tool."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb3b5c4-8a99-4a22-b274-c5066b516687",
   "metadata": {},
   "source": [
    "## Add Documents to vectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f04d7206-f013-4c7b-9321-1ed8bb62c557",
   "metadata": {},
   "outputs": [
    {
     "ename": "GoogleGenerativeAIError",
     "evalue": "Error embedding content: Timeout of 60.0s exceeded, last exception: 503 failed to connect to all addresses; last error: UNKNOWN: ipv4:142.250.70.42:443: tcp handshaker shutdown",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mD:\\Dhananjay\\Pro\\DCode\\MyAIPractice\\ai_env\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py:76\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallable_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mD:\\Dhananjay\\Pro\\DCode\\MyAIPractice\\ai_env\\Lib\\site-packages\\grpc\\_interceptor.py:277\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[1;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    270\u001b[0m     request: Any,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    275\u001b[0m     compression: Optional[grpc\u001b[38;5;241m.\u001b[39mCompression] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    276\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m--> 277\u001b[0m     response, ignored_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_with_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mD:\\Dhananjay\\Pro\\DCode\\MyAIPractice\\ai_env\\Lib\\site-packages\\grpc\\_interceptor.py:332\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._with_call\u001b[1;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[0;32m    329\u001b[0m call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interceptor\u001b[38;5;241m.\u001b[39mintercept_unary_unary(\n\u001b[0;32m    330\u001b[0m     continuation, client_call_details, request\n\u001b[0;32m    331\u001b[0m )\n\u001b[1;32m--> 332\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, call\n",
      "File \u001b[1;32mD:\\Dhananjay\\Pro\\DCode\\MyAIPractice\\ai_env\\Lib\\site-packages\\grpc\\_channel.py:440\u001b[0m, in \u001b[0;36m_InactiveRpcError.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"See grpc.Future.result.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 440\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mD:\\Dhananjay\\Pro\\DCode\\MyAIPractice\\ai_env\\Lib\\site-packages\\grpc\\_interceptor.py:315\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._with_call.<locals>.continuation\u001b[1;34m(new_details, request)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 315\u001b[0m     response, call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_thunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_method\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_credentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_wait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_compression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _UnaryOutcome(response, call)\n",
      "File \u001b[1;32mD:\\Dhananjay\\Pro\\DCode\\MyAIPractice\\ai_env\\Lib\\site-packages\\grpc\\_channel.py:1198\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.with_call\u001b[1;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[0;32m   1192\u001b[0m (\n\u001b[0;32m   1193\u001b[0m     state,\n\u001b[0;32m   1194\u001b[0m     call,\n\u001b[0;32m   1195\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking(\n\u001b[0;32m   1196\u001b[0m     request, timeout, metadata, credentials, wait_for_ready, compression\n\u001b[0;32m   1197\u001b[0m )\n\u001b[1;32m-> 1198\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_end_unary_response_blocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Dhananjay\\Pro\\DCode\\MyAIPractice\\ai_env\\Lib\\site-packages\\grpc\\_channel.py:1006\u001b[0m, in \u001b[0;36m_end_unary_response_blocking\u001b[1;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[0;32m   1005\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1006\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _InactiveRpcError(state)\n",
      "\u001b[1;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"failed to connect to all addresses; last error: UNKNOWN: ipv4:142.250.70.42:443: tcp handshaker shutdown\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_message:\"failed to connect to all addresses; last error: UNKNOWN: ipv4:142.250.70.42:443: tcp handshaker shutdown\", grpc_status:14, created_time:\"2025-03-29T15:04:08.8392621+00:00\"}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mServiceUnavailable\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32mD:\\Dhananjay\\Pro\\DCode\\MyAIPractice\\ai_env\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:144\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 144\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n",
      "File \u001b[1;32mD:\\Dhananjay\\Pro\\DCode\\MyAIPractice\\ai_env\\Lib\\site-packages\\google\\api_core\\timeout.py:130\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m remaining_timeout\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Dhananjay\\Pro\\DCode\\MyAIPractice\\ai_env\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py:78\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[1;31mServiceUnavailable\u001b[0m: 503 failed to connect to all addresses; last error: UNKNOWN: ipv4:142.250.70.42:443: tcp handshaker shutdown",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRetryError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mD:\\Dhananjay\\Pro\\DCode\\MyAIPractice\\ai_env\\Lib\\site-packages\\langchain_google_genai\\embeddings.py:227\u001b[0m, in \u001b[0;36mGoogleGenerativeAIEmbeddings.embed_documents\u001b[1;34m(self, texts, batch_size, task_type, titles, output_dimensionality)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 227\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_embed_contents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mBatchEmbedContentsRequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequests\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequests\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mD:\\Dhananjay\\Pro\\DCode\\MyAIPractice\\ai_env\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py:1392\u001b[0m, in \u001b[0;36mGenerativeServiceClient.batch_embed_contents\u001b[1;34m(self, request, model, requests, retry, timeout, metadata)\u001b[0m\n\u001b[0;32m   1391\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[1;32m-> 1392\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1393\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1394\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1395\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1396\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1397\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1399\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Dhananjay\\Pro\\DCode\\MyAIPractice\\ai_env\\Lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[1;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[0;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[1;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Dhananjay\\Pro\\DCode\\MyAIPractice\\ai_env\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:293\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    290\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[0;32m    292\u001b[0m )\n\u001b[1;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Dhananjay\\Pro\\DCode\\MyAIPractice\\ai_env\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:153\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m     \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43msleep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Dhananjay\\Pro\\DCode\\MyAIPractice\\ai_env\\Lib\\site-packages\\google\\api_core\\retry\\retry_base.py:221\u001b[0m, in \u001b[0;36m_retry_error_helper\u001b[1;34m(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[0;32m    216\u001b[0m     final_exc, source_exc \u001b[38;5;241m=\u001b[39m exc_factory_fn(\n\u001b[0;32m    217\u001b[0m         error_list,\n\u001b[0;32m    218\u001b[0m         RetryFailureReason\u001b[38;5;241m.\u001b[39mTIMEOUT,\n\u001b[0;32m    219\u001b[0m         original_timeout,\n\u001b[0;32m    220\u001b[0m     )\n\u001b[1;32m--> 221\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msource_exc\u001b[39;00m\n\u001b[0;32m    222\u001b[0m _LOGGER\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying due to \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, sleeping \u001b[39m\u001b[38;5;132;01m{:.1f}\u001b[39;00m\u001b[38;5;124ms ...\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(error_list[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], next_sleep)\n\u001b[0;32m    224\u001b[0m )\n",
      "\u001b[1;31mRetryError\u001b[0m: Timeout of 60.0s exceeded, last exception: 503 failed to connect to all addresses; last error: UNKNOWN: ipv4:142.250.70.42:443: tcp handshaker shutdown",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mGoogleGenerativeAIError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Add to vectorDB\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstores\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Chroma\n\u001b[1;32m----> 6\u001b[0m vectorstore \u001b[38;5;241m=\u001b[39m \u001b[43mChroma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlegal-contract-chroma\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_embedding_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./chroma_langchain_db\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m#  save data locally\u001b[39;49;00m\n\u001b[0;32m     11\u001b[0m \n\u001b[0;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m vector_store_retriever \u001b[38;5;241m=\u001b[39m vectorstore\u001b[38;5;241m.\u001b[39mas_retriever()\n",
      "File \u001b[1;32mD:\\Dhananjay\\Pro\\DCode\\MyAIPractice\\ai_env\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:887\u001b[0m, in \u001b[0;36mChroma.from_documents\u001b[1;34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[0;32m    885\u001b[0m texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m    886\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m--> 887\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    892\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    894\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    896\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    897\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    898\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Dhananjay\\Pro\\DCode\\MyAIPractice\\ai_env\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:843\u001b[0m, in \u001b[0;36mChroma.from_texts\u001b[1;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[0;32m    835\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mchromadb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatch_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_batches\n\u001b[0;32m    837\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m create_batches(\n\u001b[0;32m    838\u001b[0m         api\u001b[38;5;241m=\u001b[39mchroma_collection\u001b[38;5;241m.\u001b[39m_client,  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[0;32m    839\u001b[0m         ids\u001b[38;5;241m=\u001b[39mids,\n\u001b[0;32m    840\u001b[0m         metadatas\u001b[38;5;241m=\u001b[39mmetadatas,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    841\u001b[0m         documents\u001b[38;5;241m=\u001b[39mtexts,\n\u001b[0;32m    842\u001b[0m     ):\n\u001b[1;32m--> 843\u001b[0m         \u001b[43mchroma_collection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    844\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    845\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    846\u001b[0m \u001b[43m            \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    848\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    849\u001b[0m     chroma_collection\u001b[38;5;241m.\u001b[39madd_texts(texts\u001b[38;5;241m=\u001b[39mtexts, metadatas\u001b[38;5;241m=\u001b[39mmetadatas, ids\u001b[38;5;241m=\u001b[39mids)\n",
      "File \u001b[1;32mD:\\Dhananjay\\Pro\\DCode\\MyAIPractice\\ai_env\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:277\u001b[0m, in \u001b[0;36mChroma.add_texts\u001b[1;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(texts)\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 277\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embedding_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metadatas:\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;66;03m# fill metadatas with empty dicts if somebody\u001b[39;00m\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;66;03m# did not specify metadata for all texts\u001b[39;00m\n\u001b[0;32m    281\u001b[0m     length_diff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(texts) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(metadatas)\n",
      "File \u001b[1;32mD:\\Dhananjay\\Pro\\DCode\\MyAIPractice\\ai_env\\Lib\\site-packages\\langchain_google_genai\\embeddings.py:231\u001b[0m, in \u001b[0;36mGoogleGenerativeAIEmbeddings.embed_documents\u001b[1;34m(self, texts, batch_size, task_type, titles, output_dimensionality)\u001b[0m\n\u001b[0;32m    227\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mbatch_embed_contents(\n\u001b[0;32m    228\u001b[0m             BatchEmbedContentsRequest(requests\u001b[38;5;241m=\u001b[39mrequests, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m    229\u001b[0m         )\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 231\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m GoogleGenerativeAIError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError embedding content: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    232\u001b[0m     embeddings\u001b[38;5;241m.\u001b[39mextend([\u001b[38;5;28mlist\u001b[39m(e\u001b[38;5;241m.\u001b[39mvalues) \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m result\u001b[38;5;241m.\u001b[39membeddings])\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "\u001b[1;31mGoogleGenerativeAIError\u001b[0m: Error embedding content: Timeout of 60.0s exceeded, last exception: 503 failed to connect to all addresses; last error: UNKNOWN: ipv4:142.250.70.42:443: tcp handshaker shutdown"
     ]
    }
   ],
   "source": [
    "# Add to vectorDB\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    collection_name=\"legal-contract-chroma\",\n",
    "    embedding=get_embedding_model(),\n",
    "    persist_directory=\"./chroma_langchain_db\",  #  save data locally\n",
    "\n",
    ")\n",
    "\n",
    "vector_store_retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe68a50-f067-4bf4-b625-13b448264e64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "192234b5-7d46-42ae-99ec-1973dd7eb427",
   "metadata": {},
   "source": [
    "### Create Retriever Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cb375e-32de-4c6c-b19a-9e7decdaf8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "# Create a retriever tool to extract clauses\n",
    "retriever_tool = create_retriever_tool(\n",
    "    vector_store_retriever,  \n",
    "    \"retrieve_information\",\n",
    "    \"Search and return information\"\n",
    ")\n",
    "\n",
    "# Define tools list\n",
    "tools = [retriever_tool]\n",
    "\n",
    "# Example usage (if integrated in an agent-based workflow)\n",
    "result = retriever_tool.invoke(\"What is the meaning of 'Adverse Event' in the context of this agreement?\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc34e083-2096-45ed-a01f-7bf40ee7c5f1",
   "metadata": {},
   "source": [
    "# Agent State"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8fafa5-f185-4c64-a125-7e60b040709c",
   "metadata": {},
   "source": [
    "We will define a graph.\n",
    "\n",
    "A `state` object that it passes around to each `node`.\n",
    "\n",
    "Our state will be a list of `messages`.\n",
    "\n",
    "Each `node` in our graph will append to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaf1744-d04c-4a79-b17a-6e979a5a1f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Sequence\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    # The add_messages function defines how an update should be processed\n",
    "    # Default is to replace. add_messages says \"append\"\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2559eaea-4916-442c-82ee-a1540abdc39c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6dd8cd2-1aed-4fb5-b368-b155b9de2aa3",
   "metadata": {},
   "source": [
    "# Nodes and Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0fa49b-5f67-4a67-9320-402075e82179",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af71b96-5ae4-42e4-835e-d2a63df806cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "\n",
    "# Load and display an image\n",
    "img = Image.open(\"Nodes_and_Edges.png\")  # Replace with your image path\n",
    "display(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401cc085-4134-4d44-9878-1eb32945468b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Literal, Sequence\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "from langgraph.prebuilt import tools_condition\n",
    "\n",
    "### Edges\n",
    "\n",
    "\n",
    "def grade_documents(state) -> Literal[\"generate\", \"rewrite\"]:\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        str: A decision for whether the documents are relevant or not\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK RELEVANCE---\")\n",
    "\n",
    "    # Data model\n",
    "    class grade(BaseModel):\n",
    "        \"\"\"Binary score for relevance check.\"\"\"\n",
    "\n",
    "        binary_score: str = Field(description=\"Relevance score 'yes' or 'no'\")\n",
    "\n",
    "    # LLM\n",
    "    # model = ChatOpenAI(temperature=0, model=\"gpt-4o\", streaming=True)\n",
    "    llm = get_llm_model()\n",
    "    \n",
    "\n",
    "    # LLM with tool and validation\n",
    "    llm_with_tool = llm.with_structured_output(grade)\n",
    "\n",
    "    # Prompt\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "        Here is the retrieved document: \\n\\n {context} \\n\\n\n",
    "        Here is the user question: {question} \\n\n",
    "        If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "        Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\",\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "    )\n",
    "\n",
    "    # Chain\n",
    "    chain = prompt | llm_with_tool\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    question = messages[0].content\n",
    "    docs = last_message.content\n",
    "\n",
    "    scored_result = chain.invoke({\"question\": question, \"context\": docs})\n",
    "\n",
    "    score = scored_result.binary_score\n",
    "\n",
    "    if score == \"yes\":\n",
    "        print(\"---DECISION: DOCS RELEVANT---\")\n",
    "        return \"generate\"\n",
    "\n",
    "    else:\n",
    "        print(\"---DECISION: DOCS NOT RELEVANT---\")\n",
    "        print(score)\n",
    "        return \"rewrite\"\n",
    "\n",
    "\n",
    "### Nodes\n",
    "\n",
    "\n",
    "def agent(state):\n",
    "    \"\"\"\n",
    "    Invokes the agent model to generate a response based on the current state. Given\n",
    "    the question, it will decide to retrieve using the retriever tool, or simply end.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated state with the agent response appended to messages\n",
    "    \"\"\"\n",
    "    print(\"---CALL AGENT---\")\n",
    "    messages = state[\"messages\"]\n",
    "    # model = ChatOpenAI(temperature=0, streaming=True, model=\"gpt-4-turbo\")\n",
    "    llm = get_llm_model()\n",
    "    model = llm.bind_tools(tools)\n",
    "    response = model.invoke(messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "def rewrite(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated state with re-phrased question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "\n",
    "    msg = [\n",
    "        HumanMessage(\n",
    "            content=f\"\"\" \\n \n",
    "    Look at the input and try to reason about the underlying semantic intent / meaning. \\n \n",
    "    Here is the initial question:\n",
    "    \\n ------- \\n\n",
    "    {question} \n",
    "    \\n ------- \\n\n",
    "    Formulate an improved question: \"\"\",\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Grader\n",
    "    # model = ChatOpenAI(temperature=0, model=\"gpt-4-0125-preview\", streaming=True)\n",
    "    llm = get_llm_model()\n",
    "    \n",
    "    response = llm.invoke(msg)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state\n",
    "\n",
    "    Returns:\n",
    "         dict: The updated state with re-phrased question\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    docs = last_message.content\n",
    "\n",
    "    # Prompt\n",
    "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "    # LLM\n",
    "    # llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0, streaming=True)\n",
    "    llm = get_llm_model()\n",
    "\n",
    "    # Post-processing\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    # Chain\n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    # Run\n",
    "    response = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "print(\"*\" * 20 + \"Prompt[rlm/rag-prompt]\" + \"*\" * 20)\n",
    "prompt = hub.pull(\"rlm/rag-prompt\").pretty_print()  # Show what the prompt looks like"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a26166-5cfd-4bd3-a95e-e105614abf54",
   "metadata": {},
   "source": [
    "# Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4610f8a-b220-48ca-a7fd-aa145f33b86e",
   "metadata": {},
   "source": [
    "Start with an agent, call_model\n",
    "\n",
    "Agent make a decision to call a function\n",
    "\n",
    "If so, then action to call tool (retriever)\n",
    "\n",
    "Then call agent with the tool output added to messages (state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81724336-64a8-408f-8d2e-343724dbdb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Define the nodes we will cycle between\n",
    "workflow.add_node(\"agent\", agent)  # agent\n",
    "retrieve = ToolNode([retriever_tool])\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieval\n",
    "workflow.add_node(\"rewrite\", rewrite)  # Re-writing the question\n",
    "workflow.add_node(\n",
    "    \"generate\", generate\n",
    ")  # Generating a response after we know the documents are relevant\n",
    "\n",
    "\n",
    "# Call agent node to decide to retrieve or not\n",
    "workflow.add_edge(START, \"agent\")\n",
    "\n",
    "# Decide whether to retrieve\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    # Assess agent decision\n",
    "    tools_condition,\n",
    "    {\n",
    "        # Translate the condition outputs to nodes in our graph\n",
    "        \"tools\": \"retrieve\",\n",
    "        END: END,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Edges taken after the `action` node is called.\n",
    "workflow.add_conditional_edges(\n",
    "    \"retrieve\",\n",
    "    # Assess agent decision\n",
    "    grade_documents,\n",
    ")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "workflow.add_edge(\"rewrite\", \"agent\")\n",
    "\n",
    "# Compile\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37798f7b-96c3-4dcd-a68d-27be829239f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph(xray=True).draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b4a0f9-7265-40ca-84d6-691cb21ed796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "query  = input(\"Aks question to your docuement: \")\n",
    "\n",
    "inputs = {\n",
    "    \"messages\": [\n",
    "        (\"user\", query),\n",
    "    ]\n",
    "}\n",
    "for output in graph.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint.pprint(f\"Output from node '{key}':\")\n",
    "        pprint.pprint(\"---\")\n",
    "        pprint.pprint(value, indent=2, width=80, depth=None)\n",
    "    pprint.pprint(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adf9cca-7e4d-43d7-bf5e-9cb97560ec44",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a legal document assistant specializing in retrieving and summarizing contract-related information. \n",
    "You assist users by searching for relevant clauses, answering questions based on available contractual content, \n",
    "and providing legally sound explanations.\n",
    "\n",
    "### **Guidelines for Answering Queries:**\n",
    "- Always prioritize **retrieving relevant information** from the document before generating responses.\n",
    "- If the retrieved information directly answers the query, provide a **clear and concise** response.\n",
    "- If relevant information is **not found**, politely state that the document does not contain the requested details.\n",
    "- For **general legal questions** (outside the document’s scope), provide a disclaimer:  \n",
    "  *'I am designed to assist with contract-specific queries. For general legal advice, please consult a legal expert.'*\n",
    "- Maintain **neutral, professional, and factual** language at all times.\n",
    "\n",
    "### **Handling User Queries:**\n",
    "- If the query is **directly related** to the document (contract terms, clauses, obligations), extract and summarize the relevant content.\n",
    "- If the query is **indirectly related** (contract law concepts, legal definitions), provide general guidance but avoid giving legal advice.\n",
    "- If the query is **irrelevant** (e.g., sports, technology, general AI), politely inform the user that the chatbot focuses on contract-related queries.\n",
    "\n",
    "### **Example Interactions:**\n",
    "❓ **User:** *What are the confidentiality obligations in this contract?*  \n",
    "✅ **AI:** *The contract defines confidentiality in Section 13.1. It states that both parties must protect sensitive information and use it only for permitted purposes...*  \n",
    "\n",
    "❓ **User:** *Can you explain the difference between a supply agreement and a service contract?*  \n",
    "⚠️ **AI:** *I can provide general guidance, but for specific legal advice, consult a professional. A supply agreement typically governs the sale of goods, whereas a service contract focuses on service obligations...*  \n",
    "\n",
    "❓ **User:** *Who won the FIFA World Cup in 2022?*  \n",
    "🚫 **AI:** *I'm designed to assist with contract-related queries. Let me know if you have any legal document questions!*  \n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f5c57f-48c4-4e9b-8113-5cbdd3ee508a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class ChatRequest(BaseModel):\n",
    "    session_id: str  \n",
    "    message: str\n",
    "\n",
    "class ChatResponse(BaseModel):\n",
    "    response: str\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c0d35f-24b3-4fd0-81ab-a3a69ef35472",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "# Chat Service Class\n",
    "class ChatService:\n",
    "    def __init__(self):\n",
    "        self.graph = graph\n",
    "        self.chat_histories = [SystemMessage(content=SYSTEM_PROMPT)]  # Store chat history persistently\n",
    "\n",
    "    def handle_message(self, message: str):\n",
    "        \"\"\"Processes user input and maintains chat history.\"\"\"\n",
    "        \n",
    "        # Add user message to history\n",
    "        self.chat_history.append(HumanMessage(content=message))\n",
    "\n",
    "        # Prepare state with messages\n",
    "        state = AgentState(messages=self.chat_history)\n",
    "\n",
    "        # Invoke LangGraph\n",
    "        next_state = self.graph.invoke(state)\n",
    "\n",
    "        print(next_state)\n",
    "\n",
    "        # Extract AI response\n",
    "        if next_state[\"messages\"]:\n",
    "            ai_response = next_state[\"messages\"][-1].content\n",
    "            self.chat_history.append(AIMessage(content=ai_response))  # Store AI response persistently\n",
    "            return ai_response\n",
    "\n",
    "        return \"No response generated.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b81232-f9b7-4d68-a4ee-2090677f19bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store chat histories (simulating user sessions)\n",
    "chat_histories = {}\n",
    "\n",
    "# Initialize chatbot\n",
    "chatbot = ChatService()\n",
    "\n",
    "\n",
    "def chat_endpoint(request: ChatRequest):\n",
    "    \"\"\"Handles user messages and maintains session-based chat history.\"\"\"\n",
    "\n",
    "    session_id = request.get(\"session_id\")\n",
    "\n",
    "    # Initialize chat history for a new session\n",
    "    if session_id not in chat_histories:\n",
    "        chat_histories[session_id] = chatbot.chat_histories  # Start with system prompt\n",
    "\n",
    "    # Pass session-specific history to chatbot\n",
    "    chatbot.chat_history = chat_histories[session_id]\n",
    "\n",
    "    # Process user message\n",
    "    response = chatbot.handle_message(request.get(\"message\"))\n",
    "\n",
    "    # Update session chat history\n",
    "    chat_histories[session_id] = chatbot.chat_history  \n",
    "\n",
    "    return {\"response\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dba37a1-46b9-403a-b246-0d5290c46214",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dde734f-1595-4bf2-9021-2e252d6f4c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"What is the role of the Quality Agreement in API manufacturing\"\n",
    "\n",
    "session_id = \"1\"\n",
    "request = {\"session_id\":session_id, \"message\": user_input}\n",
    "\n",
    "response = chat_endpoint(request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0890a5-a9cd-4252-9dd0-8e69830b8398",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0663f5d8-42c1-4f68-bf50-5d56f5163f7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d06983-9247-4a77-b4de-2a58392d3277",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513ae5fb-3d82-4acb-9c37-f3194bf548b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7e8d1a-960f-4919-8b94-c39935ac5243",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "958693e5-0bb9-4889-8395-72e886866a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatService:\n",
    "    def __init__(self):\n",
    "        self.graph = graph\n",
    "        self.chat_histories = {}  # Dictionary to maintain session-wise chat history\n",
    "\n",
    "    def handle_message(self, session_id: str, message: str):\n",
    "        \"\"\"Processes user input while maintaining session-based chat history.\"\"\"\n",
    "        \n",
    "        # Initialize session history if not exists\n",
    "        if session_id not in self.chat_histories:\n",
    "            self.chat_histories[session_id] = [SystemMessage(content=SYSTEM_PROMPT)]\n",
    "\n",
    "        # Add user message to session history\n",
    "        self.chat_histories[session_id].append(HumanMessage(content=message))\n",
    "\n",
    "        # Prepare state with messages\n",
    "        state = AgentState(messages=self.chat_histories[session_id])\n",
    "\n",
    "        # Invoke LangGraph\n",
    "        next_state = self.graph.invoke(state)\n",
    "\n",
    "        # Extract AI response\n",
    "        if next_state[\"messages\"]:\n",
    "            ai_response = next_state[\"messages\"][-1].content\n",
    "            self.chat_histories[session_id].append(AIMessage(content=ai_response))  # Store AI response\n",
    "            return ai_response\n",
    "\n",
    "        return \"No response generated.\"\n",
    "\n",
    "    def reset_chat_history(self, session_id: str):\n",
    "        \"\"\"Clears the chat history for a specific session.\"\"\"\n",
    "        self.chat_histories[session_id] = [SystemMessage(content=SYSTEM_PROMPT)]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d38b06b-bdff-4f57-ab28-19bd8d72064d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd9953c-e908-4760-9cae-8df436b55dc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "67a883a7-aad5-47b2-a9c0-51b66fe3c6d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9188ce15-ca1b-4703-ad35-ac6ef2af7635",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4bf1f3b4-3878-455c-bd71-b46a5b6cc3f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SYSTEM_PROMPT' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m chat_histories \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Initialize chatbot\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m chatbot \u001b[38;5;241m=\u001b[39m \u001b[43mChatService\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpydantic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseModel\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mChatRequest\u001b[39;00m(BaseModel):\n",
      "Cell \u001b[1;32mIn[23], line 7\u001b[0m, in \u001b[0;36mChatService.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph \u001b[38;5;241m=\u001b[39m graph\n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_history \u001b[38;5;241m=\u001b[39m [SystemMessage(content\u001b[38;5;241m=\u001b[39m\u001b[43mSYSTEM_PROMPT\u001b[49m)]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SYSTEM_PROMPT' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Store chat histories (simulating user sessions)\n",
    "chat_histories = {}\n",
    "\n",
    "# Initialize chatbot\n",
    "chatbot = ChatService()\n",
    "\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class ChatRequest(BaseModel):\n",
    "    session_id: str  \n",
    "    message: str\n",
    "\n",
    "class ChatResponse(BaseModel):\n",
    "    response: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c25ed5-4728-4f25-9118-66d73c5642b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1434f5-1fa7-4697-8056-a4cc50f9fa4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1065ea5-493a-401e-a214-9fac0e9753b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "541d2a80-b8fa-4d0e-bbd1-546427692da0",
   "metadata": {},
   "source": [
    "### Two sets of questions for testing your RAG pipeline:  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadc7f7f-aa74-4e85-aa8c-3f9d92136925",
   "metadata": {},
   "source": [
    "\n",
    "### **1. Questions Related to the Given Document (API Commercial Supply Agreement)**\n",
    "These questions will test how well your RAG system retrieves relevant information from the document.\n",
    "\n",
    "#### **Factual Questions:**\n",
    "1. What is the \"API Commercial Supply Agreement,\" and who are the involved parties?  \n",
    "2. What is the meaning of \"Adverse Event\" in the context of this agreement?  \n",
    "3. What are the obligations of the supplier under the \"Minimum Purchase Requirement\" clause?  \n",
    "4. How does the agreement define \"Confidential Information\"?  \n",
    "5. What are the responsibilities of the supplier in case of an \"API Nonconformity\"?  \n",
    "6. What does the term \"Change of Control\" refer to in the agreement?  \n",
    "7. What are the key responsibilities under the \"Regulatory Inspections\" section?  \n",
    "8. How are \"API Prices\" determined according to the agreement?  \n",
    "9. What steps should be taken if a dispute arises regarding API quality?  \n",
    "10. What is the role of the \"Quality Agreement\" in API manufacturing?  \n",
    "\n",
    "#### **Analytical/Reasoning Questions:**\n",
    "11. If a supplier fails to meet a Purchase Order deadline, what consequences are outlined in the contract?  \n",
    "12. How does the agreement ensure compliance with \"Current Good Manufacturing Practices\" (cGMPs)?  \n",
    "13. What protections are in place for confidential information shared between parties?  \n",
    "14. How does the agreement define the responsibilities for \"Regulatory Matters\"?  \n",
    "15. In what scenarios can the agreement be terminated?  \n",
    "\n",
    "---\n",
    "\n",
    "### **2. Questions NOT Related to the Given Document (General RAG System Testing)**\n",
    "These questions will test how your RAG system handles irrelevant or out-of-context queries.\n",
    "\n",
    "#### **Unrelated Factual Questions:**\n",
    "16. What is the history of the Securities Exchange Act of 1934?  \n",
    "17. How does the FDA regulate dietary supplements?  \n",
    "18. What is the process for patenting a pharmaceutical drug in the U.S.?  \n",
    "19. What are the key components of a commercial contract under U.S. law?  \n",
    "20. How do international trade agreements affect pharmaceutical supply chains?  \n",
    "\n",
    "#### **Trick/Confusing Questions:**\n",
    "21. What are the main differences between quantum computing and classical computing?  \n",
    "22. Who won the FIFA World Cup in 2022?  \n",
    "23. What are the latest advancements in artificial intelligence for contract analysis?  \n",
    "24. How does blockchain technology impact supply chain management?  \n",
    "25. What is the role of the European Medicines Agency (EMA) in drug approval?  \n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d153411-9266-4e03-944a-498a616476d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249724bc-8b3e-441b-b6dd-b4039cd253bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db787b5-3b6b-473d-9af3-06576fcf61e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf9280d-3e05-4b22-aa98-2f1ce85bf816",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09ae372-ca43-4ebe-9c42-b4ee2a789742",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b02beea-eedd-4ecc-9ffa-85ca96cd81b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "100.28.982.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a86c95-0202-4c27-ac57-d29342155fcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c20882-5694-4170-80fa-e72c24f84c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" I address validation\n",
    "\n",
    "4 octes \n",
    "\n",
    "0 to 255\n",
    "\n",
    "can not begin with zero \n",
    "\n",
    "if number begings with zero - zero should be onlynumber in that ocate\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ec286a6-af4d-452f-9072-3658044e8193",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc31d14e-d108-46a7-80a9-f29b8c90c701",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f556eb-3614-4100-9428-160af90247f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a6286ea9-08a5-4bd0-ba06-1aef3e9b23a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_ip(in_ip):\n",
    "\n",
    "    result = \"NOT VALID\"\n",
    "    \n",
    "    ip_list = in_ip.split(\".\")\n",
    "    # print(ip_list)\n",
    "    # print(len(ip_list))\n",
    "    \n",
    "\n",
    "    res_list = []\n",
    "\n",
    "    if len(ip_list) == 4:\n",
    "        for d in ip_list:\n",
    "            if 0<=int(d)<=255:\n",
    "                print(\"first test passed\")\n",
    "                if d.startswith(\"0\"):\n",
    "                    if d == \"0\":\n",
    "                        print(\"ZERO test passed\")\n",
    "                        \n",
    "                        res_list.append(True)\n",
    "                        \n",
    "                    else:\n",
    "                        print(\"ZERO test FAILED\")\n",
    "                        \n",
    "                        # print(res_list)\n",
    "                        res_list.append(False)\n",
    "                else:\n",
    "                    res_list.append(True)\n",
    "            else:\n",
    "                # print(res_list)\n",
    "                print(\"first test failed\")\n",
    "                res_list.append(False)\n",
    "    else:\n",
    "        print(\"MAIN test FAILED\")\n",
    "        res_list.append(False)\n",
    "                \n",
    "    print(res_list) \n",
    "    if all(res_list):\n",
    "        result = \"VALID\"\n",
    "    else:\n",
    "        result = \"NOT VALID\"\n",
    "                \n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d560b760-22b4-4b13-93f0-fa0172d52d38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ef71ee1e-a79c-4456-b493-1ee17d505239",
   "metadata": {},
   "outputs": [],
   "source": [
    "ip1 = \"245.0.155.10\"  # valid\n",
    "\n",
    "ip2 = \"145.0.135.9\"  # valid \n",
    "\n",
    "ip3 = \"133.022.10.12\"   # Invalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1cd46633-aab8-4782-8450-5023b0168d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first test passed\n",
      "first test passed\n",
      "ZERO test FAILED\n",
      "first test passed\n",
      "first test passed\n",
      "[True, False, True, True]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'NOT VALID'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_valid_ip(ip3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06006bd6-674e-4638-b2cf-509302a4d7b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24176ce2-7eff-4a75-b1d9-e939c060a0ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbfaf40-fa6c-4102-bb0b-2cda41f7abc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b713e6bd-0aad-43ae-82d9-96a36398f86d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62e0885-10e7-4190-b860-16296c9e0141",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a74964-ca9f-45a0-8d2e-f84b8381b512",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b57411-a6d7-4da2-96ba-894708e84426",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ai_env)",
   "language": "python",
   "name": "ai_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
